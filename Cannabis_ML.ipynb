{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc1e612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kimiazand/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kimiazand/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kimiazand/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# For model-building\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Bag of Words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# For word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pysbd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignoring warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e928962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from a CSV file\n",
    "data = pd.read_csv(mimic-iii-random-500-subjects-cannabis-use-keyword-search-sentence-results-annotated-all-sorted.csv', sep=',')\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['match sentence'], data['Label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d57b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrames for the training and testing sets\n",
    "Train=pd.DataFrame(pd.Series(X_train).array,pd.Series(y_train).array)\n",
    "Test=pd.DataFrame(pd.Series(X_test).array,pd.Series(y_test).array)\n",
    "\n",
    "# Saving the testing set to a CSV file\n",
    "path='/split/' \n",
    "Test.to_csv(path+'cannabis_Test.csv')\n",
    "\n",
    "# Saving the training set to a CSV file\n",
    "Train.to_csv(path+'cannbis_Train.csv') \n",
    "\n",
    "# Reading the testing and training sets from the CSV files\n",
    "path='/split/'\n",
    "test_set=pd.read_csv(path+'cannabis_Test.csv')\n",
    "train_set=pd.read_csv(path+'cannbis_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ca9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function named \"seg\" that takes a string argument named \"data\"\n",
    "def seg(data):\n",
    "    # create a new instance of the Segmenter class from pysbd, configured for the English language\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    # use the segmenter to split the input string into a list of sentences\n",
    "    return seg.segment(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e837f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_train = []\n",
    "seg_test = []\n",
    "\n",
    "# Iterate over the values in the X_train Series object and segment each string using the \"seg\" function, then append the segmented data to seg_train list\n",
    "for value in X_train:\n",
    "    seg_train.append(seg(value))\n",
    "\n",
    "# Iterate over the values in the X_test Series object and segment each string using the \"seg\" function, then append the segmented data to seg_test list\n",
    "for value in X_test:\n",
    "    seg_test.append(seg(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb65c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# Define a function named \"get_wordnet_pos\" that takes a part of speech tag as input and returns the corresponding WordNet POS tag\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ  # adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB  # verb\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN  # noun\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV  # adverb\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun if the tag is not one of the above\n",
    "    \n",
    "# Define a function named \"lemmatizer\" that takes a string as input, tokenizes it into words, tags the words with their part of speech, lemmatizes each word based on its part of speech, and returns the lemmatized string as output\n",
    "def lemmatizer(string):\n",
    "    # Use the pos_tag function from the nltk library to tag the words in the input string with their part of speech\n",
    "    word_pos_tags = pos_tag(word_tokenize(string))\n",
    "    # Use a list comprehension to apply the WordNetLemmatizer to each word in the input string, using its part of speech to determine the appropriate lemmatization\n",
    "    lemmatized_words = [wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for tag in word_pos_tags]\n",
    "    # Use the \"join\" method to concatenate the lemmatized words back into a single string, separated by spaces\n",
    "    return \" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store the lemmatized and segmented versions of the training and test data\n",
    "lem_seg_train = []\n",
    "lem_seg_test = []\n",
    "\n",
    "# Iterate over each sublist in the seg_train list of lists\n",
    "for sublist in seg_train:\n",
    "    # Initialize an empty list to store the lemmatized strings for the current sublist\n",
    "    lem_sublist = []\n",
    "    # Iterate over each string in the current sublist and apply the lemmatizer function to lemmatize the string\n",
    "    for string in sublist:\n",
    "        lem_sublist.append(lemmatizer(string))\n",
    "    # Append the list of lemmatized strings for the current sublist to the lem_seg_train list of lists\n",
    "    lem_seg_train.append(lem_sublist)\n",
    "\n",
    "# Iterate over each sublist in the seg_test list of lists\n",
    "for sublist in seg_test:\n",
    "    # Initialize an empty list to store the lemmatized strings for the current sublist\n",
    "    lem_sublist = []\n",
    "    # Iterate over each string in the current sublist and apply the lemmatizer function to lemmatize the string\n",
    "    for string in sublist:\n",
    "        lem_sublist.append(lemmatizer(string))\n",
    "    # Append the list of lemmatized strings for the current sublist to the lem_seg_test list of lists\n",
    "    lem_seg_test.append(lem_sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ebc273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe called 'df1' from the 'lem_seg_train' object\n",
    "df1 = pd.DataFrame(lem_seg_train)\n",
    "\n",
    "# Create a new dataframe called 'df2' from the 'lem_seg_test' object\n",
    "df2 = pd.DataFrame(lem_seg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3bccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to join non-null values in a row into a string\n",
    "def join_columns(row):\n",
    "    return ' '.join(row.dropna().astype(str))\n",
    "\n",
    "# Apply the 'join_columns' function to each row in 'df1' to create 'X_train_df'\n",
    "X_train_df = df1.apply(join_columns, axis=1)\n",
    "\n",
    "# Apply the 'join_columns' function to each row in 'df2' to create 'X_test_df'\n",
    "X_test_df = df2.apply(join_columns, axis=1)\n",
    "\n",
    "# Set 'X_train' equal to 'X_train_df' and 'X_test' equal to 'X_test_df'\n",
    "X_train = X_train_df\n",
    "X_test = X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c772ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,5))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'C': [0, 2, 4, 8, 10, 12, 14, 16, 18 ,20],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# Train a logistic regression classifier using GridSearchCV to find the best hyperparameters\n",
    "clf = LogisticRegression(max_iter=10, multi_class='ovr')\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=100, n_jobs=-1)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "clf = grid_search.best_estimator_\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Print the best hyperparameters and classification report\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TF-IDF vectorizer and SVM model\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 5))\n",
    "svm = LinearSVC()\n",
    "\n",
    "# Define the pipeline with TF-IDF vectorizer and SVM model\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('svm', svm)\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'svm__penalty': ['l1', 'l2'],\n",
    "    'svm__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Define the GridSearchCV object to search over the hyperparameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Train the GridSearchCV object on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and accuracy score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
